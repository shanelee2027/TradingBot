{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as alpaca\n",
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.historical.stock import StockHistoricalDataClient\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = True\n",
    "\n",
    "# initialize API from API keys in .env\n",
    "load_dotenv()\n",
    "\n",
    "if paper:\n",
    "    api_key = os.environ['APCA-API-PAPER-KEY-ID']\n",
    "    api_secret_key = os.environ['APCA-API-PAPER-SECRET-KEY']\n",
    "    api_base_url = 'https://paper-api.alpaca.markets'\n",
    "else:\n",
    "    api_key = os.environ['APCA-API-KEY-ID']\n",
    "    api_secret_key = os.environ['APCA-API-SECRET-KEY']\n",
    "    api_base_url = 'https://api.alpaca.markets'\n",
    "\n",
    "api = alpaca.REST(api_key, api_secret_key, api_base_url)\n",
    "account = api.get_account()\n",
    "trading_client = TradingClient(api_key, api_secret_key, paper=paper)\n",
    "data_client = StockHistoricalDataClient(api_key, api_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SP500():\n",
    "    market_caps = []\n",
    "    SP500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "    #SP500 = SP500[0:20]\n",
    "    for index, row in SP500.iterrows():\n",
    "        print(f\"{index}: {row['Symbol']}\")\n",
    "        query_attempts = 0\n",
    "        market_cap = 0\n",
    "        while (True):\n",
    "            query_attempts += 1\n",
    "            if query_attempts >= 3:\n",
    "                raise Exception('query failed')\n",
    "            try:\n",
    "                market_cap = yf.Ticker(row['Symbol']).info.get(\"marketCap\")\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "        market_caps.append(market_cap)\n",
    "        print(market_cap)\n",
    "        print()\n",
    "        time.sleep(1)\n",
    "    SP500[\"Market Cap\"] = market_caps\n",
    "    return SP500\n",
    "\n",
    "def get_SP(n=500):\n",
    "    return pd.read_csv('SP500.csv')[0:n].sort_values(by='Symbol')\n",
    "\n",
    "def get_weekly_stock_bars(symbols, start=dt.datetime(2019, 1, 1), filename='output.csv'):\n",
    "    request_params = StockBarsRequest(\n",
    "        symbol_or_symbols=symbols,\n",
    "        timeframe=TimeFrame.Day,\n",
    "        start=start,\n",
    "        adjustment='all'\n",
    "    )\n",
    "    bars = repeated_get_stock_bars(request_params)\n",
    "    df = bars.df\n",
    "    df = df[df.index.get_level_values('timestamp').day_name() == 'Friday']\n",
    "    df['log return'] = np.log(df['close'] / df['close'].groupby(level=0).shift(1))\n",
    "    df = df.dropna(subset=['log return'])\n",
    "    df = df[['close', 'log return']]\n",
    "    df = df.swaplevel('timestamp', 'symbol')\n",
    "    df = df.sort_index(level=['timestamp', 'symbol'], ascending=[True, True])\n",
    "\n",
    "    valid_symbols = df.groupby('symbol').size()\n",
    "    valid_symbols = valid_symbols[valid_symbols == df.index.get_level_values(level=0).nunique()].index\n",
    "    df = df[df.index.get_level_values(level=1).isin(valid_symbols)]\n",
    "    df.to_csv(filename)\n",
    "\n",
    "def repeated_get_stock_bars(request_params):\n",
    "    query_attempts = 0\n",
    "    while (True):\n",
    "        query_attempts += 1\n",
    "        if query_attempts >= 3:\n",
    "            raise Exception('query failed')\n",
    "        try:\n",
    "            return data_client.get_stock_bars(request_params)\n",
    "        except:\n",
    "            time.sleep(3)\n",
    "\n",
    "def train_and_store_models(test_start_date='2022-01-01', test_end_date='2023-01-01'):\n",
    "    models = {}\n",
    "    df = pd.read_csv('output.csv')\n",
    "    df = df[(df['timestamp'] < test_start_date) | (df['timestamp'] > test_end_date)]\n",
    "    for symbol in df['symbol'].unique():\n",
    "        print(f\"Training {symbol} model...\")\n",
    "        df_pivot = df.pivot(index='timestamp', columns='symbol', values='log return')\n",
    "        y = df_pivot[symbol].values\n",
    "        X = df_pivot.drop(symbol, axis=1)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        # Train/test split\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "        X_train = X_scaled\n",
    "        y_train = y\n",
    "\n",
    "        # Adjusted range for alpha and l1_ratio\n",
    "        param_grid = {\n",
    "            'alpha': [0.005, 0.01, 0.05, 0.1],  # Try smaller alpha values\n",
    "            'l1_ratio': [0.05, 0.1, 0.2, 0.35, 0.5, 0.65, 0.8, 0.9, 0.95]     # Try a mix between Ridge and Lasso\n",
    "        }\n",
    "\n",
    "        # Set up the ElasticNet model\n",
    "        elasticnet = ElasticNet()\n",
    "\n",
    "        # Perform grid search with 5-fold cross-validation\n",
    "        grid_search = GridSearchCV(elasticnet, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters found\n",
    "        best_alpha = grid_search.best_params_['alpha']\n",
    "        best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "        print(f\"Best alpha: {best_alpha}, Best l1_ratio: {best_l1_ratio}\")\n",
    "\n",
    "        # Fit the ElasticNet model with the best parameters found\n",
    "        model = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # # Predictions on the test set\n",
    "        # y_pred = model.predict(X_test)\n",
    "\n",
    "        # # Evaluate with Mean Squared Error (MSE)\n",
    "        # mse = mean_squared_error(y_test, y_pred)\n",
    "        # print(f'Mean Squared Error with best parameters: {mse:.4f}')\n",
    "\n",
    "        models[symbol] = model\n",
    "\n",
    "        # # Save the predictions to a CSV file\n",
    "        # results = pd.DataFrame({'True Values': y_test, 'Predictions': y_pred})\n",
    "        # results.to_csv('predictions.csv', index=False)\n",
    "    joblib.dump(models, 'stock_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SP50 = get_SP(100)\n",
    "#get_weekly_stock_bars(SP50['Symbol'], start=dt.datetime(2016, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_store_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('model.pkl')\n",
    "# Coefficients for each feature\n",
    "coefficients = loaded_model.coef_\n",
    "\n",
    "# Intercept (bias term)\n",
    "intercept = loaded_model.intercept_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Assuming `X_train.columns` contains the stock symbols\n",
    "coef_df = pd.DataFrame({\n",
    "    'Stock Symbol': X.columns, \n",
    "    'Coefficient': loaded_model.coef_\n",
    "})\n",
    "\n",
    "# Sort by absolute value to highlight the most impactful stocks\n",
    "coef_df_sorted = coef_df.reindex(coef_df['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "coef_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "loaded_models = joblib.load('stock_models.pkl')\n",
    "\n",
    "# Access a specific model\n",
    "nvda_model = loaded_models['NVDA']\n",
    "aapl_model = loaded_models['AAPL']\n",
    "\n",
    "# Example prediction\n",
    "y_pred_nvda = nvda_model.predict(X_nvda)\n",
    "y_pred_aapl = aapl_model.predict(X_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "# Define the start and end date for the test period\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Split the data based on custom date range\n",
    "train_data = df[(df['timestamp'] < start_date) | (df['timestamp'] > end_date)]  # Training data before the test period\n",
    "test_data = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]  # Test data within the specified period\n",
    "display(train_data)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
